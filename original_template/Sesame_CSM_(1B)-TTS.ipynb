{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priya170205/composio/blob/master/original_template/Sesame_CSM_(1B)-TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIY6MPKBXFxk"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nykI2AHYXFxl"
      },
      "source": [
        "Placeholder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhu0k4CcXFxl"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install yt-dlp for downloading YouTube audio\n",
        "# Install pydub for audio conversion\n",
        "!pip install yt-dlp pydub soundfile\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roz1omwuXGzH",
        "outputId": "91b1ed02-2ea0-489c-f3e8-4e18795cd5ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.8.27-py3-none-any.whl.metadata (175 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Downloading yt_dlp-2025.8.27-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.8.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the YouTube video URL\n",
        "youtube_url = \"https://www.youtube.com/watch?v=FHIml6cRz0k&ab_channel=SandeepMaheshwari\"\n"
      ],
      "metadata": {
        "id": "Z3oCU4pIXGxn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Download audio as MP3\n",
        "!yt-dlp -x --audio-format mp3 {youtube_url}\n",
        "\n",
        "# List all mp3 files downloaded\n",
        "mp3_files = [f for f in os.listdir() if f.endswith(\".mp3\")]\n",
        "print(\"Downloaded MP3 files:\", mp3_files)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnYDkEboXGtv",
        "outputId": "944723b9-7c9b-499f-c2c9-330b645d09d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=FHIml6cRz0k\n",
            "[youtube] FHIml6cRz0k: Downloading webpage\n",
            "[youtube] FHIml6cRz0k: Downloading tv simply player API JSON\n",
            "[youtube] FHIml6cRz0k: Downloading tv client config\n",
            "[youtube] FHIml6cRz0k: Downloading player 6742b2b9-main\n",
            "[youtube] FHIml6cRz0k: Downloading tv player API JSON\n",
            "[info] FHIml6cRz0k: Downloading 1 format(s): 251\n",
            "[download] Destination: 99.99% Don’t Know The Self ｜ By Sandeep Maheshwari ｜ Motivational Video Hindi [FHIml6cRz0k].webm\n",
            "\u001b[K[download] 100% of   19.76MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m23.34MiB/s\u001b[0m\n",
            "[ExtractAudio] Destination: 99.99% Don’t Know The Self ｜ By Sandeep Maheshwari ｜ Motivational Video Hindi [FHIml6cRz0k].mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first downloaded mp3 file\n",
        "original_mp3 = mp3_files[0]\n",
        "new_mp3 = \"audio.mp3\"\n",
        "\n",
        "os.rename(original_mp3, new_mp3)\n",
        "print(f\"Renamed '{original_mp3}' to '{new_mp3}'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "v3q2aoX3Z7Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Convert to WAV\n",
        "audio = AudioSegment.from_mp3(new_mp3)\n",
        "audio.export(\"audio.wav\", format=\"wav\")\n",
        "print(\"Converted to audio.wav\")\n"
      ],
      "metadata": {
        "id": "LigEf-6abc1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment, effects\n",
        "\n",
        "audio = AudioSegment.from_file(\"audio.wav\")\n",
        "normalized_audio = effects.normalize(audio)\n",
        "normalized_audio.export(\"audio_normalized.wav\", format=\"wav\")\n"
      ],
      "metadata": {
        "id": "zBhcPQXfxkdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Load your 30-min audio\n",
        "audio = AudioSegment.from_file(\"audio.wav\")\n",
        "os.makedirs(\"chunks\", exist_ok=True)\n",
        "# Split into 30-second chunks (or 10-second if you want more samples)\n",
        "chunk_length_ms = 10 * 1000  # 30 seconds\n",
        "chunks = []\n",
        "overlap_ms = 2000         # 2-second overlap\n",
        "\n",
        "for start_ms in range(0, len(audio), chunk_length_ms - overlap_ms):\n",
        "    chunk = audio[start_ms:start_ms + chunk_length_ms]\n",
        "\n",
        "for i, start_ms in enumerate(range(0, len(audio), chunk_length_ms)):\n",
        "    chunk = audio[start_ms:start_ms + chunk_length_ms]\n",
        "    chunk_name = f\"chunks/voice_chunk_{i}.wav\"\n",
        "    chunk.export(chunk_name, format=\"wav\")\n",
        "    chunks.append(chunk_name)\n",
        "\n",
        "print(\"Chunks created:\", chunks[:5], \"...\")  # preview first 5 chunks\n"
      ],
      "metadata": {
        "id": "Zg4_GnF_d31h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total chunks:\", len(chunks))\n"
      ],
      "metadata": {
        "id": "RLoyyzKDd3yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faster-whisper\n",
        "\n",
        "from faster_whisper import WhisperModel\n",
        "import os # Import the os module\n",
        "\n",
        "model = WhisperModel(\"large\", device=\"cuda\", compute_type=\"float16\")  # use \"cuda\" if GPU\n",
        "chunk_folder = \"chunks\"\n",
        "transcripts = []\n",
        "\n"
      ],
      "metadata": {
        "id": "3_0R29p0g447"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "chunk_folder = \"chunks\"\n",
        "\n",
        "# List all .wav files\n",
        "chunk_files = sorted([f for f in os.listdir(chunk_folder) if f.endswith(\".wav\")])\n",
        "\n",
        "# Transcription loop with progress\n",
        "transcripts = []\n",
        "for idx, file in enumerate(chunk_files):\n",
        "    path = os.path.join(chunk_folder, file)\n",
        "    print(f\"Starting transcription of chunk {idx+1}/{len(chunk_files)}: {file} ...\")\n",
        "    result = model.transcribe(path)\n",
        "    text = result.text\n",
        "    segments = result.segments\n",
        "\n",
        "    # Optional: print word-level timestamps for this chunk\n",
        "    for seg in segments:\n",
        "        print(\"Segment:\", seg.text)\n",
        "        for w in seg.words:\n",
        "            print(f\"Word: {w.word}, start: {w.start:.2f}s, end: {w.end:.2f}s\")\n",
        "\n",
        "\n",
        "    # Save results\n",
        "    transcripts.append({\n",
        "        \"audio\": path,\n",
        "        \"text\": text,\n",
        "        \"segments\": [\n",
        "            {\n",
        "                \"segment_text\": seg.text,\n",
        "                \"words\": [{\"word\": w.word, \"start\": w.start, \"end\": w.end} for w in seg.words]\n",
        "            } for seg in segments\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    print(f\"Chunk {idx+1} done ✅\")\n",
        "\n",
        "# Save JSON and JSONL\n",
        "with open(\"voice_dataset.json\", \"w\") as f:\n",
        "    json.dump(transcripts, f, indent=2)\n",
        "\n",
        "with open(\"voice_dataset.jsonl\", \"w\") as f:\n",
        "    for entry in transcripts:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "FS2ofxOHhcl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##test\n",
        "import json\n",
        "\n",
        "# Load the JSON dataset\n",
        "with open(\"voice_dataset.json\", \"r\") as f:\n",
        "    transcripts = json.load(f)\n",
        "\n",
        "print(f\"Total chunks: {len(transcripts)}\")\n"
      ],
      "metadata": {
        "id": "zCFnnzRyhcim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "# Choose a few random chunks to check\n",
        "for i in [1,4,3]:  # first 3 chunks\n",
        "    print(f\"--- Chunk {i} ---\")\n",
        "    print(\"Transcript:\", transcripts[i][\"text\"])\n",
        "    display(Audio(transcripts[i][\"audio\"]))\n"
      ],
      "metadata": {
        "id": "U3HKzFCMeMSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Qnbvy9ZeMPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3rx0oy4deMLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "\n",
        "audio_data, sampling_rate = sf.read(\"audio.wav\")\n",
        "print(\"Audio length (samples):\", len(audio_data))\n",
        "print(\"Sampling rate:\", sampling_rate)\n"
      ],
      "metadata": {
        "id": "C72I4d1zZ7M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(\"audio.wav\")\n"
      ],
      "metadata": {
        "id": "_RMv6w9MZ7KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9lEvNU3XFxl"
      },
      "outputs": [],
      "source": [
        "# Placeholder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-LqUgufi7JU"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install unsloth\n",
        "!pip install unsloth[colab-new] torch accelerate peft transformers bitsandbytes trl xformers --upgrade"
      ],
      "metadata": {
        "id": "hEQHvfSAcggm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "from transformers import CsmForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "model, processor = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/csm-1b\",\n",
        "    max_seq_length= 2048, # Choose any for long context!\n",
        "    dtype = None, # Leave as None for auto-detection\n",
        "    auto_model = CsmForConditionalGeneration,\n",
        "    load_in_4bit = False, # Select True for 4bit - reduces memory usage\n",
        ")"
      ],
      "metadata": {
        "id": "fvsssxGucgYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_R0oWSwi7JV"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "from transformers import CsmForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "model, processor = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/csm-1b\",\n",
        "    max_seq_length= 2048, # Choose any for long context!\n",
        "    dtype = None, # Leave as None for auto-detection\n",
        "    auto_model = CsmForConditionalGeneration,\n",
        "    load_in_4bit = False, # Select True for 4bit - reduces memory usage\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPTVeI9ni7JY"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dk5azmwi7Jb"
      },
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cflyBsb9i7Je"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeCRvPI5i7Jh"
      },
      "outputs": [],
      "source": [
        "#@title Dataset Prep functions\n",
        "from datasets import load_dataset, Audio, Dataset\n",
        "import os\n",
        "from transformers import AutoProcessor\n",
        "processor = AutoProcessor.from_pretrained(\"unsloth/csm-1b\")\n",
        "\n",
        "raw_ds = load_dataset(\"MrDragonFox/Elise\", split=\"train\")\n",
        "\n",
        "# Getting the speaker id is important for multi-speaker models and speaker consistency\n",
        "speaker_key = \"source\"\n",
        "if \"source\" not in raw_ds.column_names and \"speaker_id\" not in raw_ds.column_names:\n",
        "    print(\"Unsloth: No speaker found, adding default \\\"source\\\" of 0 for all examples\")\n",
        "    new_column = [\"0\"] * len(raw_ds)\n",
        "    raw_ds = raw_ds.add_column(\"source\", new_column)\n",
        "elif \"source\" not in raw_ds.column_names and \"speaker_id\" in raw_ds.column_names:\n",
        "    speaker_key = \"speaker_id\"\n",
        "\n",
        "target_sampling_rate = 24000\n",
        "raw_ds = raw_ds.cast_column(\"audio\", Audio(sampling_rate=target_sampling_rate))\n",
        "\n",
        "def preprocess_example(example):\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": str(example[speaker_key]),\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": example[\"text\"]},\n",
        "                {\"type\": \"audio\", \"path\": example[\"audio\"][\"array\"]},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        model_inputs = processor.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=True,\n",
        "            return_dict=True,\n",
        "            output_labels=True,\n",
        "            text_kwargs = {\n",
        "                \"padding\": \"max_length\", # pad to the max_length\n",
        "                \"max_length\": 256, # this should be the max length of audio\n",
        "                \"pad_to_multiple_of\": 8,\n",
        "                \"padding_side\": \"right\",\n",
        "            },\n",
        "            audio_kwargs = {\n",
        "                \"sampling_rate\": 24_000,\n",
        "                \"max_length\": 240001, # max input_values length of the whole dataset\n",
        "                \"padding\": \"max_length\",\n",
        "            },\n",
        "            common_kwargs = {\"return_tensors\": \"pt\"},\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example with text '{example['text'][:50]}...': {e}\")\n",
        "        return None\n",
        "\n",
        "    required_keys = [\"input_ids\", \"attention_mask\", \"labels\", \"input_values\", \"input_values_cutoffs\"]\n",
        "    processed_example = {}\n",
        "    # print(model_inputs.keys())\n",
        "    for key in required_keys:\n",
        "        if key not in model_inputs:\n",
        "            print(f\"Warning: Required key '{key}' not found in processor output for example.\")\n",
        "            return None\n",
        "\n",
        "        value = model_inputs[key][0]\n",
        "        processed_example[key] = value\n",
        "\n",
        "\n",
        "    # Final check (optional but good)\n",
        "    if not all(isinstance(processed_example[key], torch.Tensor) for key in processed_example):\n",
        "         print(f\"Error: Not all required keys are tensors in final processed example. Keys: {list(processed_example.keys())}\")\n",
        "         return None\n",
        "\n",
        "    return processed_example\n",
        "\n",
        "processed_ds = raw_ds.map(\n",
        "    preprocess_example,\n",
        "    remove_columns=raw_ds.column_names,\n",
        "    desc=\"Preprocessing dataset\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTIxxQqai7Jj"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface  `Trainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G03LvbfOi7Jm"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    train_dataset = processed_ds,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01, # Turn this on if overfitting\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-pEAe-cLi7Jo"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szZOUYHgi7Js"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SjWdtI8zi7Jv"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWCCkIXyi7Jy"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "632Tt7-LXFxn"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "import soundfile as sf\n",
        "\n",
        "text = \"We just finished fine tuning a text to speech model... and it's pretty good!\"\n",
        "speaker_id = 0\n",
        "inputs = processor(f\"[{speaker_id}]{text}\", add_special_tokens=True).to(\"cuda\")\n",
        "audio_values = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=125, # 125 tokens is 10 seconds of audio, for longer speech increase this\n",
        "    # play with these parameters to tweak results\n",
        "    # depth_decoder_top_k=0,\n",
        "    # depth_decoder_top_p=0.9,\n",
        "    # depth_decoder_do_sample=True,\n",
        "    # depth_decoder_temperature=0.9,\n",
        "    # top_k=0,\n",
        "    # top_p=1.0,\n",
        "    # temperature=0.9,\n",
        "    # do_sample=True,\n",
        "    #########################################################\n",
        "    output_audio=True\n",
        ")\n",
        "audio = audio_values[0].to(torch.float32).cpu().numpy()\n",
        "sf.write(\"example_without_context.wav\", audio, 24000)\n",
        "display(Audio(audio, rate=24000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndUX2-dHXFxn"
      },
      "outputs": [],
      "source": [
        "text = \"Sesame is a super cool TTS model which can be fine tuned with Unsloth.\"\n",
        "\n",
        "speaker_id = 0\n",
        "# Another equivalent way to prepare the inputs\n",
        "conversation = [\n",
        "    {\"role\": str(speaker_id), \"content\": [{\"type\": \"text\", \"text\": text}]},\n",
        "]\n",
        "audio_values = model.generate(\n",
        "    **processor.apply_chat_template(\n",
        "        conversation,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "    ).to(\"cuda\"),\n",
        "    max_new_tokens=125, # 125 tokens is 10 seconds of audio, for longer speech increase this\n",
        "    # play with these parameters to tweak results\n",
        "    # depth_decoder_top_k=0,\n",
        "    # depth_decoder_top_p=0.9,\n",
        "    # depth_decoder_do_sample=True,\n",
        "    # depth_decoder_temperature=0.9,\n",
        "    # top_k=0,\n",
        "    # top_p=1.0,\n",
        "    # temperature=0.9,\n",
        "    # do_sample=True,\n",
        "    #########################################################\n",
        "    output_audio=True\n",
        ")\n",
        "audio = audio_values[0].to(torch.float32).cpu().numpy()\n",
        "sf.write(\"example_without_context.wav\", audio, 24000)\n",
        "display(Audio(audio, rate=24000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQdBpAsIXFxn"
      },
      "source": [
        "#### Voice and style consistency\n",
        "\n",
        "Sesame CSM's power comes from providing audio context for each speaker. Let's pass a sample utterance from our dataset to ground speaker identity and style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkauMCh0XFxn"
      },
      "outputs": [],
      "source": [
        "speaker_id = 0\n",
        "\n",
        "utterance = raw_ds[3][\"audio\"][\"array\"]\n",
        "utterance_text = raw_ds[3][\"text\"]\n",
        "text = \"Sesame is a super cool TTS model which can be fine tuned with Unsloth.\"\n",
        "\n",
        "# CSM will fill in the audio for the last text.\n",
        "# You can even provide a conversation history back in as you generate new audio\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": str(speaker_id), \"content\": [{\"type\": \"text\", \"text\": utterance_text},{\"type\": \"audio\", \"path\": utterance}]},\n",
        "    {\"role\": str(speaker_id), \"content\": [{\"type\": \"text\", \"text\": text}]},\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "        conversation,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "    )\n",
        "audio_values = model.generate(\n",
        "    **inputs.to(\"cuda\"),\n",
        "    max_new_tokens=125, # 125 tokens is 10 seconds of audio, for longer text increase this\n",
        "    # play with these parameters to tweak results\n",
        "    # depth_decoder_top_k=0,\n",
        "    # depth_decoder_top_p=0.9,\n",
        "    # depth_decoder_do_sample=True,\n",
        "    # depth_decoder_temperature=0.9,\n",
        "    # top_k=0,\n",
        "    # top_p=1.0,\n",
        "    # temperature=0.9,\n",
        "    # do_sample=True,\n",
        "    #########################################################\n",
        "    output_audio=True\n",
        ")\n",
        "audio = audio_values[0].to(torch.float32).cpu().numpy()\n",
        "sf.write(\"example_with_context.wav\", audio, 24000)\n",
        "display(Audio(audio, rate=24000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJIc7oYdi7J2"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfyvxgUEi7J3"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "processor.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# processor.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eve_aZkli7J9"
      },
      "source": [
        "### Saving to float16\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bAEpXHHi7J-"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", processor, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", processor, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", processor, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", processor, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    processor.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    processor.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf782d1c"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "unsloth_dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}